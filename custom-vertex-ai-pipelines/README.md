# Model Deployment Using Custom GCP Vertex AI Pipeline

This repository contains code to create and deploy a custom GCP Vertex AI pipeline. We have used [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) from Kaggle as our use case. The idea is to highlight the workflow of deploying custom machine learning pipelines using GCP Vertex AI, therefore the same steps can apply to any other use case. The pipeline consists of the following components:

- Data Ingestion
- Data Preprocessing
- Train-Test Split
- Model Training
- Model Evaluation
- Model Deployment to Vertex AI endpoint

In addition to the pipeline, the code also creates custom training and serving container images.

## Prerequisites

Before running the pipeline, the following prerequisites must be completed:

- **Select or create a GCP project:** Follow the instructions [here](https://developers.google.com/workspace/guides/create-project) to create a GCP project. Also, enable [billing](https://cloud.google.com/billing/docs/how-to/modify-project#enable_billing_for_a_project) for your project.

- **Activate Vertex AI on GCP:** To use Vertex AI, you will need to enable the Vertex AI API on your GCP account. Follow the instructions in the [official documentation](https://cloud.google.com/vertex-ai/docs/start) to activate Vertex AI.

- **Create a Google Storage Bucket:** You will need to create a Google Storage Bucket to store the data and artifacts generated by the pipeline. Follow the instructions in the [official documentation](https://cloud.google.com/storage/docs/creating-buckets) to create a bucket.


## Usage

To run the pipeline, follow these steps:

1. **Clone the repository** to your GCP VM instance.
2. **Create a virtual Python environment:** You should create a virtual Python environment to run the code in this repository. Follow the instructions [here](https://virtualenv.pypa.io/en/stable/user_guide.html) to create a virtual environment using `virtualenv`.
3. **Activate the virtual environment:** Activate the virtual environment you created in Step 2.
```bash
source /path/to/venv/bin/activate
```
4. **Install the required packages in your activated virtual environment:**
```bash
pip install google-cloud-aiplatform
pip install kfp
```
5. **Select your virtual environment as your kernel in the Jupyter Notebook (custom_vertex_ai_pipelines.ipynb):** You can use [ipykernel](https://janakiev.com/blog/jupyter-virtual-envs/) to add the virtual environment to your notebook.
6. **Build custom training and serving container images:** Before you can run the notebook/pipeline, you would also need to build custom training and serving container images. Follow the instructions in the [article] to build your custom container images.

Once all the steps above are complete, you should be able to run the Jupyter Notebook (custom_vertex_ai_pipelines.ipynb). When the pipeline has finished running, you can make predictions by calling the Vertex AI endpoint created in the pipeline. An example is included towards the end of the notebook demonstrating how to call the endpoint and make predictions.


## Code Structure

The repository has the following structure:

- `custom_vertex_ai_pipelines.ipynb`: The main Jupyter Notebook used to run the custom Vertex AI pipeline.
- `src`: A directory containing the following subdirectories,
  - `data_preprocessing`: Contains scripts for cleaning and preprocessing the raw data. 
  - `feature_engineering`: Contains script for handling rare categorical variables and performing one-hot encoding.
  - `feature_selection`: Contains script for selecting the most important features for modelling using recursive feature elimination.
  - `modelling`: Contains script for building, training, and evaluating the machine learning model.
  - `utils`: Contains utility script for extracting image data.
- `requirements.txt`: Contains Python packages required to run the pipeline and will be used for creating a custom Docker container image for training.
- `Dockerfile`: Contains the instructions for building the custom training Docker container image.
- `sh_0.1_build_image.sh`: Shell script to build the custom training container image.
- `sh_0.2_push_image.sh`: Shell script to push the custom training container image to Google Artifact Registry.
- `serving_container`: Contains the files required to build the custom serving container image.
    - `Dockerfile`: Contains the instructions to create the custom serving container image.
    - `sh_0.1_build_image.sh`: Shell script to build the custom serving container image.
    - `sh_0.2_push_image.sh`: Shell script to push the custom serving container image to Google Artifact Registry.
    - `app.py`: Flask web application that serves as the entry point for the custom serving container image.
    - `predict.py`: Python script that loads the trained model and provides a `predict` function to make predictions on new data.
    - `requirements.txt`: File listing the Python dependencies needed to run the Flask application and `predict.py`.
    - `src` folder: Folder containing the Python code needed for unpickling the trained model in `predict.py`.


## Further Reading

For a more detailed description and step-by-step guidelines, please refer to [my Medium article](https://medium.com/my-article) where I explain how to create and deploy a custom GCP Vertex AI pipeline.
